{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport random\nimport numpy as np\nfrom glob import glob\nimport PIL\nimport matplotlib.pyplot as plt\nimport warnings\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:28:01.077518Z","iopub.execute_input":"2023-09-24T17:28:01.078239Z","iopub.status.idle":"2023-09-24T17:28:01.086755Z","shell.execute_reply.started":"2023-09-24T17:28:01.078204Z","shell.execute_reply":"2023-09-24T17:28:01.085621Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Get the dataset\n\nThe LoL Dataset has been created for low-light image enhancement. It provides 485 images for training and 15 for testing. Each image pair in the dataset consits of a low-light input image and its corresponding well-exposed reference image.","metadata":{}},{"cell_type":"code","source":"data = '/kaggle/input/lol-dataset/lol_dataset/'\nprint(os.listdir(data))","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:28:14.699584Z","iopub.execute_input":"2023-09-24T17:28:14.699948Z","iopub.status.idle":"2023-09-24T17:28:14.713946Z","shell.execute_reply.started":"2023-09-24T17:28:14.699918Z","shell.execute_reply":"2023-09-24T17:28:14.712691Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"['eval15', 'our485']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Creating a Tensorflow Dataset\n\nWe use 300 image pairs from the LoL Dataset's training set for training, and we use the remaining 185 image pairs for validation. We generate random crops of size 128 x 128 from the image pairs to be used for both training and validation.","metadata":{}},{"cell_type":"code","source":"random.seed(10)\nIMAGE_SIZE = 128\nBATCH_SIZE = 4\nMAX_TRAIN_IMAGES = 300","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:28:14.756115Z","iopub.execute_input":"2023-09-24T17:28:14.756381Z","iopub.status.idle":"2023-09-24T17:28:14.761389Z","shell.execute_reply.started":"2023-09-24T17:28:14.756358Z","shell.execute_reply":"2023-09-24T17:28:14.760258Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def read_image(image_path):\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_png(image, channels=3)\n    image.set_shape([None, None, 3])\n    image = tf.cast(image, dtype=tf.float32) / 255.0\n    return image","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:28:19.631279Z","iopub.execute_input":"2023-09-24T17:28:19.631751Z","iopub.status.idle":"2023-09-24T17:28:19.638974Z","shell.execute_reply.started":"2023-09-24T17:28:19.631709Z","shell.execute_reply":"2023-09-24T17:28:19.637654Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def random_crop(low_image, enhanced_image):\n    low_image_shape = tf.shape(low_image)[:2]\n    low_w = tf.random.uniform(\n        shape=(), maxval=low_image_shape[1] - IMAGE_SIZE + 1, dtype=tf.int32\n    )\n    low_h = tf.random.uniform(\n        shape=(), maxval=low_image_shape[0] - IMAGE_SIZE + 1, dtype=tf.int32\n    )\n    enhanced_w = low_w\n    enhanced_h = low_h\n    low_image_cropped = low_image[\n        low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE\n    ]\n    enhanced_image_cropped = enhanced_image[\n        enhanced_h : enhanced_h + IMAGE_SIZE, enhanced_w : enhanced_w + IMAGE_SIZE\n    ]\n    return low_image_cropped, enhanced_image_cropped","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:28:21.251716Z","iopub.execute_input":"2023-09-24T17:28:21.252102Z","iopub.status.idle":"2023-09-24T17:28:21.259931Z","shell.execute_reply.started":"2023-09-24T17:28:21.252070Z","shell.execute_reply":"2023-09-24T17:28:21.258748Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def load_data(low_light_image_path, enhanced_image_path):\n    low_light_image = read_image(low_light_image_path)\n    enhanced_image = read_image(enhanced_image_path)\n    low_light_image, enhanced_image = random_crop(low_light_image, enhanced_image)\n    return low_light_image, enhanced_image","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:28:28.642817Z","iopub.execute_input":"2023-09-24T17:28:28.643180Z","iopub.status.idle":"2023-09-24T17:28:28.648336Z","shell.execute_reply.started":"2023-09-24T17:28:28.643150Z","shell.execute_reply":"2023-09-24T17:28:28.647329Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_dataset(low_light_images, enhanced_images):\n    dataset = tf.data.Dataset.from_tensor_slices((low_light_images, enhanced_images))\n    dataset = dataset.map(load_data, num_parallel_calls = tf.data.AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:28:32.249474Z","iopub.execute_input":"2023-09-24T17:28:32.249822Z","iopub.status.idle":"2023-09-24T17:28:32.255609Z","shell.execute_reply.started":"2023-09-24T17:28:32.249792Z","shell.execute_reply":"2023-09-24T17:28:32.254481Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Distribution of data","metadata":{}},{"cell_type":"code","source":"train_low_light_images = sorted(glob(\"/kaggle/input/lol-dataset/lol_dataset/our485/low/*\"))[:MAX_TRAIN_IMAGES]\ntrain_enhanced_images = sorted(glob(\"/kaggle/input/lol-dataset/lol_dataset/our485/high/*\"))[:MAX_TRAIN_IMAGES]\n\nval_low_light_images = sorted(glob(\"/kaggle/input/lol-dataset/lol_dataset/our485/low/*\"))[MAX_TRAIN_IMAGES:]\nval_enhanced_images = sorted(glob(\"/kaggle/input/lol-dataset/lol_dataset/our485/high/*\"))[MAX_TRAIN_IMAGES:]\n\ntest_low_light_images = sorted(glob(\"/kaggle/input/lol-dataset/lol_dataset/eval15/low/*\"))\ntest_enhanced_images = sorted(glob(\"/kaggle/input/lol-dataset/lol_dataset/eval15/high/*\"))","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:28:35.257888Z","iopub.execute_input":"2023-09-24T17:28:35.258595Z","iopub.status.idle":"2023-09-24T17:28:35.651705Z","shell.execute_reply.started":"2023-09-24T17:28:35.258558Z","shell.execute_reply":"2023-09-24T17:28:35.650598Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_dataset = get_dataset(train_low_light_images, train_enhanced_images)\nval_dataset = get_dataset(val_low_light_images, val_enhanced_images)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:28:37.963897Z","iopub.execute_input":"2023-09-24T17:28:37.964284Z","iopub.status.idle":"2023-09-24T17:28:44.036858Z","shell.execute_reply.started":"2023-09-24T17:28:37.964254Z","shell.execute_reply":"2023-09-24T17:28:44.035860Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(\"Train dataset: \", train_dataset)\nprint(\"Val dataset: \", val_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:28:47.722485Z","iopub.execute_input":"2023-09-24T17:28:47.722834Z","iopub.status.idle":"2023-09-24T17:28:47.728321Z","shell.execute_reply.started":"2023-09-24T17:28:47.722803Z","shell.execute_reply":"2023-09-24T17:28:47.727382Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Train dataset:  <_BatchDataset element_spec=(TensorSpec(shape=(4, None, None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(4, None, None, 3), dtype=tf.float32, name=None))>\nVal dataset:  <_BatchDataset element_spec=(TensorSpec(shape=(4, None, None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(4, None, None, 3), dtype=tf.float32, name=None))>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# MIRNet Model\n\nHere are the main features of the MIRNet model:\n\n- A feature extraction model that computes a complementary set of features across multiple spatial scales, while maintaining the original high-resolution features to preserveprecise spatial details.\n\n- A regularly repeated mechanism for information exchange, where the features across multi-resolution branches are progressively fused together for improved representation learning.\n\n- A new approach to fuse multi-scale features using a selective kernel network that dynamically combines variable receptive fields and faithfully preserves the original feature information at each spatial resolution.\n\n- A recursive residual design that progressively breaks down the input signal in order to simplify the overall learning process, and allows the construction of very deep networks.\n\n![](https://raw.githubusercontent.com/soumik12345/MIRNet/master/assets/mirnet_architecture.png)","metadata":{}},{"cell_type":"markdown","source":"## Selective Kernel Feature Fusion\n\nThe Selective Kernel Feature Fusion or SKFF module performs dynamic adjustment of\nreceptive fields via two operations: **Fuse** and **Select**. The Fuse operator generates\nglobal feature descriptors by combining the information from multi-resolution streams.\nThe Select operator uses these descriptors to recalibrate the feature maps (of different\nstreams) followed by their aggregation.\n\n**Fuse**: The SKFF receives inputs from three parallel convolution streams carrying\ndifferent scales of information. We first combine these multi-scale features using an\nelement-wise sum, on which we apply Global Average Pooling (GAP) across the spatial\ndimension. Next, we apply a channel- downscaling convolution layer to generate a compact\nfeature representation which passes through three parallel channel-upscaling convolution\nlayers (one for each resolution stream) and provides us with three feature descriptors.\n\n**Select**: This operator applies the softmax function to the feature descriptors to\nobtain the corresponding activations that are used to adaptively recalibrate multi-scale\nfeature maps. The aggregated features are defined as the sum of product of the corresponding\nmulti-scale feature and the feature descriptor.\n\n![](https://i.imgur.com/7U6ixF6.png)","metadata":{}},{"cell_type":"code","source":"def selective_kernel_feature_fusion(multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3):\n    channels = list(multi_scale_feature_1.shape)[-1]\n    combined_feature = layers.Add()([multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3])\n    \n    gap = layers.GlobalAveragePooling2D()(combined_feature)\n    channel_wise_statistics = tf.reshape(gap, shape=(-1, 1, 1, channels))\n    \n    compact_feature_representation = layers.Conv2D(filters=channels // 8, kernel_size=(1, 1), activation=\"relu\")(channel_wise_statistics)\n    \n    feature_descriptor_1 = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"softmax\")(compact_feature_representation)\n    \n    feature_descriptor_2 = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"softmax\")(compact_feature_representation)\n    \n    feature_descriptor_3 = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"softmax\")(compact_feature_representation)\n    \n    feature_1 = multi_scale_feature_1 * feature_descriptor_1\n    feature_2 = multi_scale_feature_2 * feature_descriptor_2\n    feature_3 = multi_scale_feature_3 * feature_descriptor_3\n    \n    aggregated_feature = layers.Add()([feature_1, feature_2, feature_3])\n    return aggregated_feature","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:28:55.735459Z","iopub.execute_input":"2023-09-24T17:28:55.735817Z","iopub.status.idle":"2023-09-24T17:28:55.745274Z","shell.execute_reply.started":"2023-09-24T17:28:55.735788Z","shell.execute_reply":"2023-09-24T17:28:55.744165Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Dual Attention Unit\n\nThe Dual Attention Unit or DAU is used to extract features in the convolutional streams.\nWhile the SKFF block fuses information across multi-resolution branches, we also need a\nmechanism to share information within a feature tensor, both along the spatial and the\nchannel dimensions which is done by the DAU block. The DAU suppresses less useful\nfeatures and only allows more informative ones to pass further. This feature\nrecalibration is achieved by using **Channel Attention** and **Spatial Attention**\nmechanisms.\n\nThe **Channel Attention** branch exploits the inter-channel relationships of the\nconvolutional feature maps by applying squeeze and excitation operations. Given a feature\nmap, the squeeze operation applies Global Average Pooling across spatial dimensions to\nencode global context, thus yielding a feature descriptor. The excitation operator passes\nthis feature descriptor through two convolutional layers followed by the sigmoid gating\nand generates activations. Finally, the output of Channel Attention branch is obtained by\nrescaling the input feature map with the output activations.\n\nThe **Spatial Attention** branch is designed to exploit the inter-spatial dependencies of\nconvolutional features. The goal of Spatial Attention is to generate a spatial attention\nmap and use it to recalibrate the incoming features. To generate the spatial attention\nmap, the Spatial Attention branch first independently applies Global Average Pooling and\nMax Pooling operations on input features along the channel dimensions and concatenates\nthe outputs to form a resultant feature map which is then passed through a convolution\nand sigmoid activation to obtain the spatial attention map. This spatial attention map is\nthen used to rescale the input feature map.\n\n![](https://i.imgur.com/Dl0IwQs.png)","metadata":{}},{"cell_type":"code","source":"def spatial_attention_block(input_tensor):\n    average_pooling = tf.reduce_max(input_tensor, axis=-1)\n    average_pooling = tf.expand_dims(average_pooling, axis=-1)\n    max_pooling = tf.reduce_mean(input_tensor, axis=-1)\n    max_pooling = tf.expand_dims(max_pooling, axis=-1)\n    concatenated = layers.Concatenate(axis=-1)([average_pooling, max_pooling])\n    feature_map = layers.Conv2D(1, kernel_size=(1, 1))(concatenated)\n    feature_map = tf.nn.sigmoid(feature_map)\n    return input_tensor * feature_map","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:34:29.685143Z","iopub.execute_input":"2023-09-24T17:34:29.685517Z","iopub.status.idle":"2023-09-24T17:34:29.693255Z","shell.execute_reply.started":"2023-09-24T17:34:29.685488Z","shell.execute_reply":"2023-09-24T17:34:29.691974Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def channel_attention_block(input_tensor):\n    channels = list(input_tensor.shape)[-1]\n    average_pooling = layers.GlobalAveragePooling2D()(input_tensor)\n    feature_descriptor = tf.reshape(average_pooling, shape=(-1, 1, 1, channels))\n    \n    feature_activations = layers.Conv2D(\n        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n    )(feature_descriptor)\n    \n    feature_activations = layers.Conv2D(\n        filters=channels, kernel_size=(1, 1), activation=\"sigmoid\"\n    )(feature_activations)\n    \n    return input_tensor * feature_activations","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:38:37.586150Z","iopub.execute_input":"2023-09-24T17:38:37.586518Z","iopub.status.idle":"2023-09-24T17:38:37.593465Z","shell.execute_reply.started":"2023-09-24T17:38:37.586488Z","shell.execute_reply":"2023-09-24T17:38:37.592266Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def dual_attention_unit_block(input_tensor):\n    channels = list(input_tensor.shape)[-1]\n    \n    feature_map = layers.Conv2D(\n        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n    )(input_tensor)\n    \n    feature_map = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(\n        feature_map\n    )\n    \n    channel_attention = channel_attention_block(feature_map)\n    spatial_attention = spatial_attention_block(feature_map)\n    concatenation = layers.Concatenate(axis=-1)([channel_attention, spatial_attention])\n    concatenation = layers.Conv2D(channels, kernel_size=(1, 1))(concatenation)\n    \n    return layers.Add()([input_tensor, concatenation])","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:39:15.535390Z","iopub.execute_input":"2023-09-24T17:39:15.535756Z","iopub.status.idle":"2023-09-24T17:39:15.543401Z","shell.execute_reply.started":"2023-09-24T17:39:15.535726Z","shell.execute_reply":"2023-09-24T17:39:15.542188Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Multi-Scale Residual Block\n\nThe Multi-Scale Residual Block is capable of generating a spatially-precise output by\nmaintaining high-resolution representations, while receiving rich contextual information\nfrom low-resolutions. The MRB consists of multiple (three in this paper)\nfully-convolutional streams connected in parallel. It allows information exchange across\nparallel streams in order to consolidate the high-resolution features with the help of\nlow-resolution features, and vice versa. The MIRNet employs a recursive residual design\n(with skip connections) to ease the flow of information during the learning process. In\norder to maintain the residual nature of our architecture, residual resizing modules are\nused to perform downsampling and upsampling operations that are used in the Multi-scale\nResidual Block.\n\n![](https://i.imgur.com/wzZKV57.png)","metadata":{}},{"cell_type":"code","source":"def down_sampling_module(input_tensor):\n    channels = list(input_tensor.shape)[-1]\n    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n        input_tensor)\n    \n    main_branch = layers.Conv2D(\n            channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n    )(main_branch)\n    \n    main_branch = layers.MaxPooling2D()(main_branch)\n    main_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(main_branch)\n    skip_branch = layers.MaxPooling2D()(input_tensor)\n    skip_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(skip_branch)\n    \n    return layers.Add()([skip_branch, main_branch])","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:46:26.876123Z","iopub.execute_input":"2023-09-24T17:46:26.876474Z","iopub.status.idle":"2023-09-24T17:46:26.884051Z","shell.execute_reply.started":"2023-09-24T17:46:26.876445Z","shell.execute_reply":"2023-09-24T17:46:26.883079Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def up_sampling_module(input_tensor):\n    channels = list(input_tensor.shape)[-1]\n    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n        input_tensor)\n    \n    main_branch = layers.Conv2D(\n        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n    )(main_branch)\n    \n    main_branch = layers.UpSampling2D()(main_branch)\n    main_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(main_branch)\n    skip_branch = layers.UpSampling2D()(input_tensor)\n    skip_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(skip_branch)\n    \n    return layers.Add()([skip_branch, main_branch])","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:50:31.210531Z","iopub.execute_input":"2023-09-24T17:50:31.210897Z","iopub.status.idle":"2023-09-24T17:50:31.218776Z","shell.execute_reply.started":"2023-09-24T17:50:31.210867Z","shell.execute_reply":"2023-09-24T17:50:31.217712Z"},"trusted":true},"execution_count":17,"outputs":[]}]}